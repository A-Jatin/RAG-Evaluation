## Experiment Report: Comparing Naive RAG Retrieval with HyDE Retrieval

### Objective

The goal of this experiment is to evaluate and compare the performance of Naive RAG (Retrieval-Augmented Generation) retrieval and HyDE (Hybrid Dense) retrieval. I aim to determine which method provides more accurate and relevant responses to user queries based on specific evaluation metrics.

### Dataset Selection

I utilized the Grounded QA dataset, curated from reports collected from Amnesty International's research repository (https://www.amnesty.org/en/research/). This dataset is suitable for our experiment as it contains complex, real-world queries and relevant documents. Each data point consists of a question, ground truth answers, retrieved contexts, and an answer based on those contexts.

### Experiment Setup

#### Retrieval Methods

1. **Naive RAG Retrieval**: This method combines a dense retriever with a generative model. It retrieves relevant documents and generates answers based on the retrieved context.
2. **HyDE Retrieval**: This method uses a language model, like ChatGPT, to create a theoretical document in response to a query. Instead of using the query and its computed vector to directly search in the vector database, HyDE generates a document that is then used for retrieval, combining both dense and sparse retrieval techniques to leverage the advantages of each.

#### Evaluation Metrics

To evaluate the performance of each retrieval method, I defined four key metrics:

1. **Faithfulness**: Measures the factual consistency of the answer with the context based on the question.
2. **Context Precision**: Measures how relevant the retrieved context is to the question, indicating the quality of the retrieval pipeline.
3. **Answer Relevancy**: Measures how relevant the answer is to the question.
4. **Context Recall**: Measures the retrieverâ€™s ability to retrieve all necessary information required to answer the question.

### Experiment Execution

I executed both Naive RAG and HyDE retrieval methods on the QA dataset. The results for each method based on the defined metrics are as follows:

**Naive RAG Retrieval**:
- Context Precision: 0.9208
- Faithfulness: 0.9533
- Answer Relevancy: 0.9619
- Context Recall: 0.9500

**HyDE Retrieval**:
- Context Precision: 0.9209
- Faithfulness: 0.9917
- Answer Relevancy: 0.9596
- Context Recall: 0.9500

### Analysis and Insights

#### Context Precision
Both methods performed similarly in terms of context precision, with HyDE slightly outperforming Naive RAG. This indicates that both methods are almost equally effective in retrieving relevant contexts for the questions.

#### Faithfulness
HyDE outperformed Naive RAG in faithfulness, suggesting that the answers generated by HyDE are more factually consistent with the retrieved contexts. This improvement can be attributed to HyDE's use of a language model to create theoretical documents, providing a more robust set of documents for answer generation.

#### Answer Relevancy
Naive RAG slightly outperformed HyDE in answer relevancy. This indicates that while HyDE provides more factually consistent answers, Naive RAG's answers are marginally more aligned with the queries.

#### Context Recall
Both methods achieved the same context recall score, indicating they are equally capable of retrieving all necessary information required to answer the questions.

### Conclusion

Both Naive RAG and HyDE retrieval methods exhibit strong performance across various metrics. HyDE's approach of using a language model to generate theoretical documents provides a slight edge in terms of faithfulness, making it more reliable for generating factually consistent answers. Naive RAG, however, demonstrates marginally better relevancy in its answers.


### Next Steps
Add more datasets to evaluate the performance of the retrieval methods across different domains and complexities.

---